"""Gemini Flash Task Decomposition Orchestrator System

This module implements a system that decomposes user queries into
flowchart-based tasks using Gemini Flash (dspy) and orchestrates them.
"""

import json
import os
import functools
from collections import deque
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Optional

import anyio
import dspy
from dotenv import load_dotenv
from dspy.primitives.assertions import assert_transform_module, backtrack_handler

# Gemini Flash 2.5 configuration
DUMMY_DIR = '/tmp/dummy_dir'
os.makedirs(DUMMY_DIR, exist_ok=True)
load_dotenv(os.path.expanduser(".env"))
gemini = dspy.LM(model="gemini/gemini-2.5-flash", temperature=0.0, max_tokens=10000)
dspy.settings.configure(lm=gemini)

# Pydantic data type definitions
PYDANTIC_SCHEMA = """
## Enum Types
class ProcessType(str, Enum):
    ROOT = "root"           # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®è¦æœ›
    PRIMITIVE = "primitive" # åŸå­çš„ã‚¿ã‚¹ã‚¯ï¼ˆåˆ†å‰²ä¸å¯ï¼‰
    COMPOUND = "compound"   # è¤‡åˆã‚¿ã‚¹ã‚¯ï¼ˆå­ãƒ—ãƒ­ã‚»ã‚¹ã‚’æŒã¤ï¼‰

class FlowType(str, Enum):
    SEQUENCE = "sequence"   # é †æ¬¡å®Ÿè¡Œ
    BRANCH = "branch"       # æ¡ä»¶åˆ†å²
    LOOP = "loop"          # ç¹°ã‚Šè¿”ã—

## Dataset Type
class Dataset(BaseModel):
    name: str = Field(..., description="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå")
    description: str = Field(..., description="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª¬æ˜")
    format: Optional[str] = Field(None, description="ãƒ‡ãƒ¼ã‚¿å½¢å¼")
    
    class Config:
        schema_extra = {
            "examples": [
                {
                    "name": "æœªæ‰¿èªè«‹æ±‚æ›¸",
                    "description": "æ‰¿èªå¾…ã¡ã®è«‹æ±‚æ›¸ãƒ‡ãƒ¼ã‚¿",
                    "format": "JSON"
                }
            ]
        }

## Process Definition
class ProcessStep(BaseModel):
    process_ref: str = Field(..., description="å‚ç…§ãƒ—ãƒ­ã‚»ã‚¹ID")
    input_mapping: Optional[Dict[str, str]] = Field(None, description="å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°")
    condition: Optional[str] = Field(None, description="å®Ÿè¡Œæ¡ä»¶")

class ProcessFlow(BaseModel):
    flow_type: FlowType = Field(..., description="ãƒ•ãƒ­ãƒ¼åˆ¶å¾¡ã‚¿ã‚¤ãƒ—")
    steps: List[ProcessStep] = Field(..., description="å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—ãƒªã‚¹ãƒˆ")
    condition: Optional[str] = Field(None, description="ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œæ¡ä»¶")
    loop_condition: Optional[str] = Field(None, description="ãƒ«ãƒ¼ãƒ—ç¶™ç¶šæ¡ä»¶")

    class Config:
        schema_extra = {
            "examples": [
                {
                    "flow_type": "sequence",
                    "steps": [
                        {"process_ref": "VALIDATE-INVOICE"},
                        {"process_ref": "CHECK-APPROVAL-AUTHORITY"},
                        {"process_ref": "EXECUTE-APPROVAL"}
                    ]
                },
                {
                    "flow_type": "branch",
                    "steps": [
                        {
                            "process_ref": "AUTO-APPROVE",
                            "condition": "é‡‘é¡ <= 10000"
                        },
                        {
                            "process_ref": "MANUAL-APPROVE",
                            "condition": "é‡‘é¡ > 10000"
                        }
                    ]
                }
            ]
        }

class ProcessNode(BaseModel):
    process_id: str = Field(..., description="ãƒ—ãƒ­ã‚»ã‚¹ä¸€æ„è­˜åˆ¥å­")
    name: str = Field(..., description="input->outputã«å¤‰æ›ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹å/ç›®çš„")
    process_type: ProcessType = Field(..., description="ãƒ—ãƒ­ã‚»ã‚¹ã‚¿ã‚¤ãƒ—")
    inputs: List[Dataset] = Field(default_factory=list, description="å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    outputs: List[Dataset] = Field(default_factory=list, description="å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    constraints: Optional[str] = Field(None, description="åˆ¶ç´„äº‹é …ãƒ»è¦ä»¶")
    process_flow: Optional[ProcessFlow] = Field(None, description="å­ãƒ—ãƒ­ã‚»ã‚¹ãƒ•ãƒ­ãƒ¼")

    class Config:
        schema_extra = {
            "examples": [
                {
                    "process_id": "APPROVE-INVOICE",
                    "name": "è«‹æ±‚æ›¸æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹",
                    "process_type": "compound",
                    "inputs": [
                        {
                            "name": "æœªæ‰¿èªè«‹æ±‚æ›¸",
                            "description": "æ‰¿èªå¾…ã¡ã®è«‹æ±‚æ›¸ãƒ‡ãƒ¼ã‚¿"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "æ‰¿èªæ¸ˆã¿è«‹æ±‚æ›¸",
                            "description": "æ‰¿èªå‡¦ç†æ¸ˆã¿ã®è«‹æ±‚æ›¸ãƒ‡ãƒ¼ã‚¿"
                        }
                    ],
                    "constraints": "24æ™‚é–“ä»¥å†…ã«æ‰¿èªå‡¦ç†ã‚’å®Œäº†ã™ã‚‹ã“ã¨"
                }
            ]
        }

class BusinessProcessDefinition(BaseModel):
    root_process: ProcessNode = Field(..., description="ãƒ«ãƒ¼ãƒˆãƒ—ãƒ­ã‚»ã‚¹")
    all_processes: Dict[str, ProcessNode] = Field(..., description="å…¨ãƒ—ãƒ­ã‚»ã‚¹å®šç¾©è¾æ›¸")
    
    def validate_references(self) -> bool:
        # å®Ÿè£…: ã™ã¹ã¦ã®process_refãŒall_processesã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        pass
    
    def get_process_hierarchy(self) -> Dict:
        # å®Ÿè£…: éšå±¤æ§‹é€ ã®å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        pass

    class Config:
        schema_extra = {
            "examples": [
                {
                    "root_process": {
                        "process_id": "INVOICE-PROCESSING",
                        "name": "è«‹æ±‚æ›¸å‡¦ç†å…¨ä½“ãƒ—ãƒ­ã‚»ã‚¹",
                        "process_type": "root",
                        "process_flow": {
                            "flow_type": "sequence",
                            "steps": [
                                {"process_ref": "RECEIVE-INVOICE"},
                                {"process_ref": "APPROVE-INVOICE"},
                                {"process_ref": "REGISTER-PAYMENT"}
                            ]
                        }
                    },
                    "all_processes": {
                        "INVOICE-PROCESSING": "...",
                        "RECEIVE-INVOICE": "...",
                        "APPROVE-INVOICE": "...",
                        "REGISTER-PAYMENT": "..."
                    }
                }
            ]
        }
"""

# Task execution instructions
TASK_INSTRUCTION = """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æŒ‡ç¤ºã‚’ã‚‚ã¨ã« ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’è£œå®Œã—ãŸBusinessProcessDefinitionã‚’ã€mermaidè¨˜æ³•ã¨ã—ã¦syntaxã®æ­£ã—ã•ã«æ³¨æ„ã—ã¦ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
* ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ãƒãƒ¼ãƒ‰ã¯å¿…ãšDatasetã§ã‚ã‚‹
* process_typeãŒroot,compoundã®å ´åˆã€ProcessNode.process_flowã¯å¿…ãšè¨˜è¿°ã—ã¦ãã ã•ã„
* ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æŒ‡ç¤ºã‚’å¿ å®Ÿã«å®ˆã‚Šã€æ—¢å­˜ã®ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã«å¯¾ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰æŒ‡ç¤ºã•ã‚Œã¦ãªã„å¤‰æ›´ã¯ã—ãªã„
* åˆæœŸã®æ®µéšã§ã¯ãƒ¬ãƒ™ãƒ«0ã¾ã§ã‚’ä¸€åº¦ã«æç¤ºã—ã¦ãã ã•ã„
    * 2å›ç›®ä»¥é™ã«ã‚ˆã‚Šè©³ç´°åŒ–ã‚’æ±‚ã‚ã‚‰ã‚ŒãŸå ´åˆã€è¿½åŠ ã™ã‚‹ãƒ¬ãƒ™ãƒ«ã¯ æ—¢å­˜ãƒ¬ãƒ™ãƒ«+1 ã¾ã§ã¨ã—ã¦ãã ã•ã„
* ProcessNode: ã“ã‚Œã«æ‰€å±ã™ã‚‹ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’åŒ…å«ã™ã‚‹subgraphã‚’ä½œæˆã™ã‚‹
    * ãŸã ã—ProcessNode.process_type == root ã®å ´åˆã¯subgraphã‚’ä½œæˆã—ãªã„ã§ãã ã•ã„
* input, output: ãã‚Œãã‚Œã‚’ãƒãƒ¼ãƒ‰ã¨ã™ã‚‹
* process_id, process_type, constraints ã¯æç”»ã—ãªã„
* process_flow: input -> (process_flowã‚’å±•é–‹ã—ãŸã‚‚ã®) -> output ã®é–¢ä¿‚æ€§ãŒã‚ã‹ã‚‹ã‚ˆã†ã«è¨˜è¿°
* process_flowã¯æœªå®šç¾©ã®ã‚‚ã®ã‚’å«ã‚ã¦è‰¯ã„ã®ã§ã€ProcessNodeä¸€ã¤ã ã‘ã‚’è¿”ã™ã‚‚ã®ã¨ã—ã¾ã™
* process_id, process_type, constraints ã¯æç”»ã—ãªã„
* ã‚³ãƒ¡ãƒ³ãƒˆã¯ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ™‚ã«å¤±æ•—ã—ãŒã¡ãªã®ã§ã€ã‚³ãƒ¡ãƒ³ãƒˆã¯å«ã‚ãªã„"""

@dataclass
class TaskResult:
    """Task processing result"""
    task_name: str
    mermaid_flowchart: str
    description: str
    subtasks: List[str]
    timestamp: str


# Validation functions for max_tokens truncation detection
def validate_mermaid_completion(mermaid_flowchart: str) -> bool:
    """Validate that mermaid flowchart is complete and not truncated"""
    if not mermaid_flowchart or len(mermaid_flowchart.strip()) == 0:
        return False
    
    # Check if mermaid code block is properly closed
    if "```mermaid" in mermaid_flowchart and not mermaid_flowchart.rstrip().endswith("```"):
        return False
    
    # Check for common truncation indicators
    truncation_indicators = [
        "...",  # Common truncation marker
        "# Error: Response truncated",
        "Response was truncated",
    ]
    
    for indicator in truncation_indicators:
        if indicator in mermaid_flowchart:
            return False
    
    return True

def validate_json_parseable(next_task_candidates: str) -> bool:
    """Validate that next task candidates can be parsed as JSON"""
    try:
        if not next_task_candidates:
            return True  # Empty is acceptable
        
        parsed = json.loads(next_task_candidates)
        return isinstance(parsed, list)  # Should be a list
    except (json.JSONDecodeError, TypeError):
        return False

def validate_response_completeness(output) -> bool:
    """Comprehensive validation of ProcessNodeDecomposer output"""
    # Basic presence checks
    if not hasattr(output, 'mermaid_flowchart') or not hasattr(output, 'next_task_candidates'):
        return False
    
    # Validate mermaid flowchart completeness
    if not validate_mermaid_completion(output.mermaid_flowchart):
        return False
    
    # Validate JSON parseability
    if not validate_json_parseable(output.next_task_candidates):
        return False
    
    # Check minimum content length (responses that are too short might be truncated)
    if len(output.mermaid_flowchart.strip()) < 20:  # Minimum reasonable mermaid content
        return False
    
    return True


class CompletionChecker(dspy.Signature):
    """ã‚¿ã‚¹ã‚¯ã®å®Œäº†åˆ¤å®š"""

    conversation_history = dspy.InputField(desc="ä¼šè©±å±¥æ­´")
    user_feedback = dspy.InputField(desc="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯")
    is_completed = dspy.OutputField(
        desc="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å•é¡Œãªãå—ã‘å…¥ã‚Œã¦ã„ã‚‹ã‹ï¼ˆtrue/falseï¼‰", type=bool
    )

class ProcessNodeDecomposer(dspy.Signature):
    """ãƒ—ãƒ­ã‚»ã‚¹ãƒãƒ¼ãƒ‰ã®åˆ†è§£ãƒ»è©³ç´°åŒ–ã€‚
    
    Pydanticãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã¨ã‚¿ã‚¹ã‚¯æŒ‡ç¤ºã‚’åˆ†é›¢ã—ã¦ã€
    æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹åˆ†è§£ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    task_instruction: str = dspy.InputField(desc="ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã®æŒ‡ç¤ºå†…å®¹")
    data_schema: str = dspy.InputField(desc="ProcessNodeã®Pydanticãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒå®šç¾©")
    query: str = dspy.InputField(desc="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã‚¯ã‚¨ãƒªãƒ»è¦æ±‚")
    existing_mermaid_flowchart: str = dspy.InputField(desc="æ—¢å­˜ã®Mermaidãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰", default="")
    context: str = dspy.InputField(desc="ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰", default="")
    query_history: str = dspy.InputField(desc="éå»ã®ã‚¯ã‚¨ãƒªå±¥æ­´ï¼ˆJSONæ–‡å­—åˆ—ï¼‰", default="[]")

    mermaid_flowchart: str = dspy.OutputField(
        desc="æ›´æ–°ã•ã‚ŒãŸæ­£ç¢ºãªsyntaxã®Mermaidãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã€‚ãƒ—ãƒ­ã‚»ã‚¹é–“ã®é–¢ä¿‚æ€§ã¨ã‚µãƒ–ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’å«ã‚€ã€‚"
    )
    flowchart_update_description: str = dspy.OutputField(
        desc="ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®æ›´æ–°å†…å®¹ã‚„å¤‰æ›´ç‚¹ã®èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆ"
    )
    next_task_candidates: str = dspy.OutputField(
        desc="æ¬¡ã«åˆ†è§£ãƒ»åˆ†æã™ã¹ãã‚¿ã‚¹ã‚¯ã®å€™è£œãƒªã‚¹ãƒˆï¼ˆJSONé…åˆ—å½¢å¼ã®æ–‡å­—åˆ—ï¼‰ã€‚ä¾‹: [\"ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰è¨­è¨ˆ\", \"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ\", \"APIè¨­è¨ˆ\"]"
    )

class ProcessNodeDecomposerWithAssertions(dspy.Module):
    """Enhanced ProcessNodeDecomposer with validation and retry logic"""
    
    def __init__(self):
        super().__init__()
        self.decomposer = dspy.Predict(ProcessNodeDecomposer)
    
    def forward(self, task_instruction: str, data_schema: str, query: str, 
                existing_mermaid_flowchart: str = "", context: str = "", 
                query_history: str = "[]"):
        
        result = self.decomposer(
            task_instruction=task_instruction,
            data_schema=data_schema,
            query=query,
            existing_mermaid_flowchart=existing_mermaid_flowchart,
            context=context,
            query_history=query_history
        )
        
        # Apply validation assertions
        dspy.Assert(
            validate_response_completeness(result),
            "Response appears to be truncated or incomplete. Mermaid flowchart should be properly formatted and JSON should be parseable.",
            target_module=self.decomposer
        )
        
        dspy.Assert(
            validate_mermaid_completion(result.mermaid_flowchart),
            "Mermaid flowchart appears to be truncated or improperly formatted.",
            target_module=self.decomposer
        )
        
        dspy.Assert(
            validate_json_parseable(result.next_task_candidates),
            "Next task candidates should be valid JSON array format.",
            target_module=self.decomposer
        )
        
        return result

class FlowchartUpdater(dspy.Module):
    """Enhanced Flowchart update module with validation and retry"""

    def __init__(self, max_retries: int = 2):
        super().__init__()
        # Create base decomposer with assertions
        base_decomposer = ProcessNodeDecomposerWithAssertions()
        
        # Transform with backtrack handler for retry logic
        self.decomposer = assert_transform_module(
            base_decomposer, 
            functools.partial(backtrack_handler, max_backtracks=max_retries)
        )

    def forward(self, query: str, existing_mermaid: str = "", context: str = "", query_history: list = None):
        if query_history is None:
            query_history = []

        try:
            result = self.decomposer(
                task_instruction=TASK_INSTRUCTION,
                data_schema=PYDANTIC_SCHEMA,
                query=query,
                existing_mermaid_flowchart=existing_mermaid,
                context=context,
                query_history=json.dumps(query_history, ensure_ascii=False)
            )

            return {
                "mermaid_flowchart": result.mermaid_flowchart,
                "flowchart_update_description": result.flowchart_update_description,
                "next_task_candidates": result.next_task_candidates
            }
        
        except Exception as e:
            print(f"âš ï¸ Flowchart update failed after retries: {str(e)}")
            # Return fallback response
            return {
                "mermaid_flowchart": f"graph TD\n    A[Error: {str(e)}]",
                "flowchart_update_description": f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "next_task_candidates": "[]"
            }


class TaskDivisionAgent:
    """Task Division Agent (using Gemini Flash)"""

    def __init__(self, cwd: str = None):
        self.conversation_history = []
        self.current_task = None
        self.is_completed = None
        self.cwd = Path(cwd) if cwd else Path(DUMMY_DIR)
        self.last_result = None

    async def process_task(
        self,
        raw_task: str,
        is_continuation: bool = False,
        orchestrator_state: dict = None,
    ) -> dict:
        """Process task with Gemini Flash"""
        print(f"ğŸ” Task processing started: is_continuation={is_continuation}")
        print(f"ğŸ” orchestrator_state: {orchestrator_state}")
        if orchestrator_state:
            print(f"ğŸ” meaningful_state: {self._has_meaningful_state(orchestrator_state)}")

        # Set task mode based on continuation flag
        if is_continuation:
            print("ğŸ”„ Continuation session")
            task = f"JSON correction feedback: {raw_task}"
        else:
            print("ğŸ†• New or context-aware session")
            task = raw_task

        self.current_task = task

        print(f"ğŸ¤– Gemini Flash processing: {task}")
        result = await self._run_process_decomposition(task, orchestrator_state)

        subtasks = self._extract_subtasks_from_signature(result)

        task_result = {
            "original_task": task,
            "status": "completed",
            "subtasks": subtasks,
            "raw_result": result,
        }
        self.last_result = task_result

        return task_result

    async def _run_process_decomposition(self, task: str, orchestrator_state: dict = None) -> dict:
        """Run process node decomposition using Gemini Flash"""
        # Truncate long tasks for display
        task_preview = task.split('\n')[0] if '\n' in task else task
        if len(task_preview) > 100:
            task_preview = task_preview[:100] + "..."
        print(f"ğŸ¤– Process decomposition with Gemini Flash: {task_preview}")
        print("=" * 50)

        if not hasattr(self, 'flowchart_updater'):
            self.flowchart_updater = FlowchartUpdater()

        context_str = ""
        existing_mermaid = ""
        query_history = self.conversation_history

        if orchestrator_state:
            context_str = json.dumps(orchestrator_state, ensure_ascii=False, indent=2)
            if "last_mermaid" in orchestrator_state:
                existing_mermaid = orchestrator_state["last_mermaid"]

        result = self.flowchart_updater(
            query=task,
            existing_mermaid=existing_mermaid,
            context=context_str,
            query_history=query_history
        )

        mermaid_flowchart = self._extract_mermaid_content(result['mermaid_flowchart'])

        print("ğŸ’¬ Gemini response completed")
        print("=" * 50)
        
        return {
            "mermaid_flowchart": mermaid_flowchart,
            "flowchart_update_description": result['flowchart_update_description'],
            "next_task_candidates": result['next_task_candidates'],
        }


    def _extract_mermaid_content(self, mermaid_text: str) -> str:
        """Extract content from mermaid code block"""
        if not mermaid_text:
            return ""
        
        # Extract content if wrapped in ```mermaid blocks
        import re
        pattern = r'```mermaid\s*\n(.*?)\n```'
        match = re.search(pattern, mermaid_text, re.DOTALL)
        if match:
            return match.group(1).strip()
        
        return mermaid_text.strip()

    def _extract_subtasks_from_signature(self, result: dict) -> list[str]:
        """Extract next task candidates from Signature Output"""
        try:
            next_task_candidates = result.get("next_task_candidates", "[]")
            if isinstance(next_task_candidates, str):
                subtasks = json.loads(next_task_candidates)
            else:
                subtasks = next_task_candidates
            
            print(f"ğŸ“‹ Subtasks extracted from Signature: {subtasks}")
            return subtasks if isinstance(subtasks, list) else []
        except json.JSONDecodeError:
            print("âš ï¸ Failed to parse JSON format task candidates")
            return []

    def _has_meaningful_state(self, orchestrator_state: dict) -> bool:
        """Check if orchestrator state contains meaningful information"""
        if not orchestrator_state:
            return False

        if orchestrator_state.get("subtasks_to_analyze") and len(orchestrator_state["subtasks_to_analyze"]) > 0:
            return True

        if orchestrator_state.get("analyzed_tasks") and len(orchestrator_state["analyzed_tasks"]) > 0:
            return True

        return False

    def _build_state_context(self, orchestrator_state: dict) -> str:
        """Build context information from orchestrator state"""
        context_parts = []

        if orchestrator_state.get("subtasks_to_analyze"):
            context_parts.append("## Subtasks Waiting for Analysis")
            for i, subtask in enumerate(orchestrator_state["subtasks_to_analyze"], 1):
                context_parts.append(f"{i}. {subtask}")

        if orchestrator_state.get("analyzed_tasks"):
            context_parts.append("\n## Completed Decomposed Tasks")
            for i, task in enumerate(orchestrator_state["analyzed_tasks"], 1):
                context_parts.append(f"{i}. {task}")

        if context_parts:
            return (
                "\n## Orchestrator State Information\n" + "\n".join(context_parts) + "\n"
            )
        return ""

    async def _save_task_result(self, task_name: str, agent_result: dict, should_integrate: bool = False):
        """Save task processing result and optionally integrate"""
        import datetime
        
        if not agent_result or 'raw_result' not in agent_result:
            return
            
        raw_result = agent_result['raw_result']
        
        task_result = TaskResult(
            task_name=task_name,
            mermaid_flowchart=raw_result.get('mermaid_flowchart', ''),
            description=raw_result.get('flowchart_update_description', ''),
            subtasks=agent_result.get('subtasks', []),
            timestamp=datetime.datetime.now().isoformat()
        )
        
        self.analyzed_task_results[task_name] = task_result
        print(f"ğŸ’¾ Saved task result for: {task_name}")
        
        # Only integrate when explicitly requested (is_completed=True)
        if should_integrate:
            integration_result = await self._integrate_new_task_incrementally(task_name, task_result)
            
            if integration_result['was_successful']:
                print(f"âœ… Integration completed for: {integration_result['task_name']}")
                print(f"ğŸ”§ Changes: {integration_result['integration_changes']}")
            else:
                print(f"âš ï¸ Integration had issues for: {integration_result['task_name']}")
                print(f"ğŸ“‹ Details: {integration_result['integration_changes']}")


class Orchestrator:
    """Orchestrator: Command center for task division and agent coordination"""

    def __init__(self):
        self.user_session_id = "session_001"
        self.active_agent: TaskDivisionAgent | None = None
        self.task_queue = deque()  # Queue for next analysis candidate tasks
        self.completion_checker = dspy.Predict(CompletionChecker)
        self.global_conversation_history = []
        self.analyzed_tasks = []  # Record completed decomposed tasks

    async def process_user_input(self, user_input: str, is_auto_subtask: bool = False) -> dict:
        """Process user input uniformly"""
        print(f"ğŸ¯ User input processing started: {user_input}")

        is_initial_task = len(self.global_conversation_history) == 0

        # Skip completion_checker for automatic subtask processing to avoid infinite loops
        if is_auto_subtask:
            print("ğŸ”„ Automatic subtask processing - skipping completion_checker")
            is_completed = False
        elif not is_initial_task:
            # If user says "no problem", mechanically judge as completed
            if user_input == "å•é¡Œãªã„":
                is_completed = True
            else:
                completion_result = self.completion_checker(
                    conversation_history="\n".join(self.global_conversation_history),
                    user_feedback=user_input,
                )
                is_completed = completion_result.is_completed.lower() == "true"
            print(f"ğŸ” Completion judgment result: {is_completed}")

            if is_completed:
                print("âœ… Judged as task completed")

                # Perform integration for completed task
                if self.active_agent and hasattr(self.active_agent, 'current_task'):
                    current_task = self.active_agent.current_task
                    if current_task and current_task in self.analyzed_task_results:
                        task_result = self.analyzed_task_results[current_task]
                        integration_result = await self._integrate_new_task_incrementally(current_task, task_result)
                        
                        if integration_result['was_successful']:
                            print(f"âœ… Integration completed for: {integration_result['task_name']}")
                            print(f"ğŸ”§ Changes: {integration_result['integration_changes']}")
                        else:
                            print(f"âš ï¸ Integration had issues for: {integration_result['task_name']}")
                            print(f"ğŸ“‹ Details: {integration_result['integration_changes']}")

                # Extract subtasks from last processing result and add to queue
                if self.active_agent and hasattr(self.active_agent, 'last_result'):
                    last_result = self.active_agent.last_result
                    if last_result and "raw_result" in last_result:
                        subtasks = self.active_agent._extract_subtasks_from_signature(last_result["raw_result"])
                        if subtasks:
                            self.add_to_queue(subtasks)
                            print(f"ğŸ“‹ Extracted {len(subtasks)} subtasks for analysis")

                return await self._process_next_task()
        else:
            is_completed = False

        print("ğŸ”„ Requesting agent processing")

        # Agent management based on processing context
        if is_auto_subtask:
            print("ğŸ†• Creating new agent for automatic subtask")
            self.active_agent = TaskDivisionAgent()
            is_continuation = False
            orchestrator_state = self._build_orchestrator_state()
        elif self.active_agent is None:
            print("ğŸ†• Creating new agent")
            self.active_agent = TaskDivisionAgent()
            is_continuation = False
            orchestrator_state = self._build_orchestrator_state()
        else:
            print("ğŸ”„ Continuing with existing agent")
            is_continuation = True
            orchestrator_state = None

        result = await self.active_agent.process_task(
            user_input,
            is_continuation=is_continuation,
            orchestrator_state=orchestrator_state,
        )

        # Skip conversation history update for automatic subtasks
        if not is_auto_subtask:
            self.global_conversation_history.append(f"User: {user_input}")
            agent_response = result.get('raw_result', {}).get('flowchart_update_description', result['status'])
            self.global_conversation_history.append(f"Agent: {agent_response}")

        return {
            "status": "agent_processing_completed",
            "message": "Agent generated response. Please review.",
            "agent_response": result,
        }

    def _build_orchestrator_state(self) -> dict:
        """Build current state of orchestrator"""
        return {
            "subtasks_to_analyze": list(self.task_queue),
            "analyzed_tasks": self.analyzed_tasks,
        }

    async def _process_next_task(self) -> dict:
        """Process next task"""
        if self.task_queue:
            next_task = self.task_queue.popleft()
            print(f"ğŸ“‹ Processing next task: {next_task}")

            # Record current task as completed before moving to next
            if self.active_agent and self.active_agent.current_task:
                self.analyzed_tasks.append(self.active_agent.current_task)

            # Reset agent for new task context
            self.active_agent = None
            return await self.process_user_input(next_task, is_auto_subtask=True)
        else:
            print("ğŸ‰ All tasks completed")

            if self.active_agent and self.active_agent.current_task:
                self.analyzed_tasks.append(self.active_agent.current_task)

            return {"status": "all_completed", "message": "All tasks have been completed"}

    def add_to_queue(self, tasks: list[str]):
        """Add analysis target tasks to queue"""
        for task in tasks:
            self.task_queue.append(task)
        print(f"ğŸ“ Added {tasks} to analysis queue")


def _format_response(raw_result: dict) -> str:
    """Format dictionary result for text display"""
    if not raw_result:
        return "Could not retrieve results"
    
    return f"""## Flowchart Update Results

### Update Details
{raw_result.get('flowchart_update_description', 'N/A')}

### Mermaid Flowchart
```mermaid
{raw_result.get('mermaid_flowchart', 'N/A')}
```"""


async def demo_orchestrator2():
    """Orchestrator demonstration"""
    orchestrator = Orchestrator()
    # Want to compare with and without algo_trade repository as context
    task = "Webã‚µã‚¤ãƒˆã‚’ä½œã‚ŠãŸã„"
    # task = "æ™‚ç³»åˆ—ã«å¯¾ã™ã‚‹äºˆæ¸¬çµæœã®éå®šå¸¸æ€§ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ã‹ã‘ã‚‹å ±é…¬é–¢æ•°ã‚’å®šç¾©ã—ãŸã„"
    result = await orchestrator.process_user_input(task)
    agent_response = result.get('agent_response', {})
    raw_result = agent_response.get('raw_result', {})
    response_text = _format_response(raw_result)
    print(f"\n1ï¸âƒ£ Agent response: {response_text}")
    print(f"\n2ï¸âƒ£ Analysis queue status: {list(orchestrator.task_queue)}")

    for task in ["å•é¡Œãªã„"] * 10:
        result = await orchestrator.process_user_input(task)
        agent_response = result.get('agent_response', {})
        raw_result = agent_response.get('raw_result', {})
        response_text = _format_response(raw_result)
        print(f"\n1ï¸âƒ£ Agent response: {response_text}")
        print(f"\n2ï¸âƒ£ Analysis queue status: {list(orchestrator.task_queue)}")
        
    while True:
        task = input("Enter new task: ")
        if task == "exit":
            break
        result = await orchestrator.process_user_input(task)
        agent_response = result.get('agent_response', {})
        raw_result = agent_response.get('raw_result', {})

async def demo_orchestrator3():
    """Orchestrator demonstration"""
    orchestrator = Orchestrator()
    while True:
        task = input("Enter new task: ")
        if task == "exit":
            break
        result = await orchestrator.process_user_input(task)
        agent_response = result.get('agent_response', {})
        raw_result = agent_response.get('raw_result', {})
        response_text = _format_response(raw_result)
        print(f"\n1ï¸âƒ£ Agent response: {response_text}")
    print(f"\n2ï¸âƒ£ Analysis queue status: {list(orchestrator.task_queue)}")

async def main():
    await demo_orchestrator2()


if __name__ == "__main__":
    anyio.run(main) 